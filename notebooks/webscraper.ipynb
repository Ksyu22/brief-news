{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hjvnu-h94kry",
    "outputId": "70c5a5d6-1763-467c-efa9-4c0ca16beac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/rriesco/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/rriesco/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from beautifulsoup4) (2.3.2.post1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tAEvjS-q3lu0"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DbtDC6194dui"
   },
   "outputs": [],
   "source": [
    "url_list = ['https://edition.cnn.com/2022/11/17/tech/twitter-employees-ultimatum-deadline/index.html',\n",
    "'https://edition.cnn.com/2022/11/17/india/modi-india-g20-influence-intl-hnk/index.html',\n",
    "'https://www.dailymail.co.uk/sport/sportsnews/article-11442203/Qatar-want-alcohol-sales-World-Cup-stadiums-BANNED.html',\n",
    "'http://edition.cnn.com/2022/11/18/us/five-things-november-18-trnd/index.html',\n",
    "'https://www.dailymail.co.uk/sport/sportsnews/article-11443573/Man-United-respond-Cristiano-Ronaldo-interview-Piers-Morgan.html',\n",
    "'https://www.dailymail.co.uk/femail/article-11443077/This-buttons-left-womens-shirts.html',\n",
    "'https://www.dailymail.co.uk/tvshowbiz/article-11443607/Lorraine-Kelly-shocks-fellow-celebs-expletive-ridden-rant-hilarious-Children-Need-sketch.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qKGoG708iOzQ"
   },
   "outputs": [],
   "source": [
    "# url = 'https://edition.cnn.com/2022/11/17/tech/twitter-employees-ultimatum-deadline/index.html'\n",
    "# url = 'https://edition.cnn.com/2022/11/17/india/modi-india-g20-influence-intl-hnk/index.html'\n",
    "# url = 'https://www.dailymail.co.uk/sport/sportsnews/article-11442203/Qatar-want-alcohol-sales-World-Cup-stadiums-BANNED.html'\n",
    "url = 'http://edition.cnn.com/2022/11/18/us/five-things-november-18-trnd/index.html'\n",
    "# url = 'https://www.dailymail.co.uk/sport/sportsnews/article-11443573/Man-United-respond-Cristiano-Ronaldo-interview-Piers-Morgan.html'\n",
    "# url = 'https://www.dailymail.co.uk/femail/article-11443077/This-buttons-left-womens-shirts.html'\n",
    "# url = 'https://www.dailymail.co.uk/tvshowbiz/article-11443607/Lorraine-Kelly-shocks-fellow-celebs-expletive-ridden-rant-hilarious-Children-Need-sketch.html'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "597T0XIW5gZS"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "d5QdicJI5tji"
   },
   "outputs": [],
   "source": [
    "# text = soup.find_all('p', class_='paragraph inline-placeholder')\n",
    "text = soup.find_all('p')\n",
    "text_ = [item.text.strip() for item in text]\n",
    "text_ = ''.join(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Pcoh3j5f3j28"
   },
   "outputs": [],
   "source": [
    "def CNN_scraper(url):\n",
    "    \"\"\"\n",
    "    Input: 'str'\n",
    "    Output: 'dict'\n",
    "    \n",
    "    The function recieve an url (must be from CNN), fetch for the html and uses BS4 to extract the paragraph\n",
    "    with the class='paragraph inline-placeholder' which contains the text. Then clean and merge the strings.\n",
    "    It returns a dictionary with the title and the text of the news\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.find_all('p', class_='paragraph inline-placeholder')\n",
    "    text = [item.text.strip() for item in text]\n",
    "    text = ''.join(text).replace('\\xa0', ' ')\n",
    "    title = soup.title.string.split('|')[0]\n",
    "\n",
    "    return {'title': title, 'article': text, 'id': 0, 'orig_id': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PgOPWP134i05"
   },
   "outputs": [],
   "source": [
    "def DailyMail_scraper(url):\n",
    "    \"\"\"\n",
    "    Input: 'str'\n",
    "    Output: 'dict'\n",
    "    \n",
    "    The function recieve an url (must be from DailyMail), fetch for the html and uses BS4 to extract the paragraph\n",
    "    with the class='paragraph inline-placeholder' which contains the text. Then clean and merge the strings.\n",
    "    It returns a dictionary with the title and the text of the news\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.find_all('p', class_='mol-para-with-font')\n",
    "    text = [item.text.strip() for item in text]\n",
    "    text = ''.join(text).replace('\\xa0', ' ')\n",
    "    title = soup.title.string.split('|')[0]\n",
    "\n",
    "    return {'title': title, 'article': text, 'id': 0, 'orig_id': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dpgbiy6aeZbc"
   },
   "outputs": [],
   "source": [
    "def General_scraper(url):\n",
    "    \"\"\"\n",
    "    Input: 'str'\n",
    "    Output: 'dict'\n",
    "    \n",
    "    The function recieve an url, fetch for the html and uses BS4 to extract the paragraph tags. Then it\n",
    "    counts the number of times that each paragraph is repeated and uses the most repeated (in a news must be text)\n",
    "    to scrape the news from the website. It returns a dictionary with the title and the text of the news.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.find_all('p')\n",
    "\n",
    "    attrs_list = [' '.join(item.attrs['class']) for item in text if len(item.attrs) > 0]\n",
    "    values, counts = np.unique(attrs_list, return_counts=True)\n",
    "    idx = np.where(counts == np.max(counts))\n",
    "    text_class = str(values[idx]).strip('[]\\'')\n",
    "\n",
    "    text = soup.find_all('p', class_=text_class)\n",
    "    text = [item.text.strip() for item in text]\n",
    "    text = ' '.join(text).replace('\\xa0', ' ')\n",
    "    title = soup.title.string.split('|')[0]\n",
    "\n",
    "    return {'title': title, 'article': text, 'id': 0, 'orig_id': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6nxc7NgefSGc"
   },
   "outputs": [],
   "source": [
    "v1 = General_scraper(url)\n",
    "v2 = DailyMail_scraper(url)\n",
    "v3 = CNN_scraper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqhCKAiGhRH8",
    "outputId": "c533f969-119f-46e8-f983-912cf0b705d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison General to DailyMail: False\n",
      "Comparison General to CNN: False\n"
     ]
    }
   ],
   "source": [
    "print('Comparison General to DailyMail:',v1['article'] == v2['article'])\n",
    "print('Comparison General to CNN:', v1['article'] == v3['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lmaB2vbhl9z",
    "outputId": "dccb835b-fd52-4f3d-a015-6cd747978ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of comparable text between scrapers: 0 and total number of url: 7\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for link in url_list:\n",
    "    v1 = General_scraper(url)\n",
    "    v2 = DailyMail_scraper(url)\n",
    "    v3 = CNN_scraper(url)\n",
    "\n",
    "    c1 = v1['article'] == v2['article']\n",
    "    c2 = v1['article'] == v3['article']\n",
    "\n",
    "    count = count + c1 + c2\n",
    "\n",
    "print(f'Total count of comparable text between scrapers: {count} and total number of url: {len(url_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw-rIkzii5pk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
