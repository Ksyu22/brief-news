{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install / Import all Python Modules Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaPSqT6fPMwt",
    "outputId": "ec96648b-6f78-4a86-f1a4-770f657bbab1"
   },
   "outputs": [],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGSP1XuGewDX",
    "outputId": "af2f8367-257a-48b9-f314-e02e8e01e0dd"
   },
   "outputs": [],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ba9EcWEhOvBP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 12:37:28.182423: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 12:37:28.182520: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kXHZoChaOu2c"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets required (Train Set, Validation Set, Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "UB7-TFkjOurC",
    "outputId": "9012fe2b-54e8-4f3a-830f-db251790634d"
   },
   "outputs": [],
   "source": [
    "# #loading the datasets\n",
    "# train_set = pd.read_csv('../raw_data/train.csv')\n",
    "# validation_set = pd.read_csv('../raw_data/validation.csv')\n",
    "# test_set = pd.read_csv('../raw_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brief_news.data.big_query import get_bq_chunk\n",
    "\n",
    "train_set = get_bq_chunk('train', 0, 100)\n",
    "validation_set = get_bq_chunk('validation', 0, 100)\n",
    "test_set = get_bq_chunk('test', 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "SGYMbKSTOudV",
    "outputId": "66d47752-305c-485d-d258-ccd2339b710b"
   },
   "outputs": [],
   "source": [
    "# droppping useless columns\n",
    "train_data = train_set.drop(['id', 'orig_id'], axis=1)[:5000]\n",
    "val_data = validation_set.drop(['id', 'orig_id'], axis=1)[:200]\n",
    "test_data = test_set.drop(['id', 'orig_id'], axis=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5Eo0oP9mOuSM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gZLUT0l6Ot9S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KARBALA, Iraq (CNN) -- A female suicide bomber apparently targeting Shiite worshippers killed at least 40 people and wounded at least 65 in Karbala on Monday, according to an Interior Ministry official. Iraqi security forces gather around the site of a car bomb explosion in Baghdad on Monday. The incident occurred one-half mile from the Imam Hussein shrine of Karbala. Karbala is a Shiite holy city, and the Imam Hussein shrine is one of Shiite Islam's holiest locations. The shrine marks the burial spot of Hussein bin Ali, the grandson of the Prophet Mohammed, who was killed in battle nearby in 680. No more information was immediately available about the blast southwest of the capital city, Baghdad. Earlier Monday, in Baghdad, a roadside bomb exploded near an Iraqi police patrol, killing one officer and wounding another, the Interior Ministry told CNN. A short time later, another roadside bomb exploded near an Iraqi police patrol on Palestine Street in eastern Baghdad, wounding four bystanders, a ministry official said. The first attack took place about 8:30 a.m. in the upscale Mansour neighborhood, where law enforcement officials have come under frequent attacks in recent weeks. Also Monday, two American soldiers were killed by a roadside bomb north of Baghdad, officials said. The incident occurred about 12:20 p.m.  as the soldiers were \"conducting a route-clearance combat operation north of Baghdad,\" according to a news release. The names of the soldiers were not immediately released. Meanwhile, U.S. Vice President Dick Cheney arrived in the Iraqi capital Monday on an unannounced visit. Cheney told reporters that the five years in Iraq since the war's start has been \"well worth the effort.\" He said he met with top Iraqi officials.  He appeared at a news conference with Gen. David Petraeus, the top U.S. commander in Iraq, and Ryan Crocker, the U.S. ambassador to the country. Cheney began a trip to the Middle East on Sunday with an official itinerary that listed stops in Oman, Saudi Arabia, Turkey, Israel and the West Bank, according to the White House. E-mail to a friend . CNN's Mohammed Tawfeeq contributed to this report.</td>\n",
       "      <td>NEW: 2 U.S. soldiers killed by roadside bomb in Baghdad on Monday, U.S. says .\\nNEW: Other roadside bombs in Baghdad kill one police officer, injure four people .\\nDeath toll rises to 40 in explosion in Karbala, official says; 65 injured .\\nExplosion was near holy shrine for Shiite Muslims, burial spot of Hussein bin Ali .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOSTON, Massachusetts (CNN) -- It all started with the flush of an automatic toilet. The terrifying sound marked the beginning of a two-year nightmare for Sarah Teres as she desperately tried to potty train her daughter Molly. Molly graduated from Potty School four days before her brother, Archer, was born. \"It was awful\" Teres said. \"We tried everything including bribery and threats.\" Teres, the mother of three from Andover, Massachusetts, hoped her middle child would be toilet trained by the time she was 2½. Two years later, the girl was still in diapers, refusing to use the bathroom. \"I was going crazy,\" Teres admitted. \"She wouldn't poop. She would hold it for days.\" At wits end, Teres enrolled Molly in the Toilet Training School at Children's Hospital Boston. \"By the time the children come in with their families, it has become a power struggle,\" explained Dr. Alison Schonwald, a pediatrician who supervises the \"poop school,\" as it's affectionately called by staffers. \"The kids kind of dig in their heels and put a line in the sand.\"  Health Minute: Watch more on the perils of potty training » . The American Academy of Pediatrics reports that most children show signs they're ready to potty train between 18 and 24 months. Doctors suggest a child may be ready to start trying if he's staying dry for at least two hours at a time during the day, walking to and from the bathroom, asking for a diaper change and asking to use the potty. For some kids, the toilet training process can take more than a year, or longer. The six-week program at Children's Hospital is one of a handful around the country. Kimberly Dunn, a pediatric nurse practitioner, has worked with some of the 450 young graduates over the years. She said most of the kids admit they are afraid to use a toilet. \"Oftentimes, the parents come in and they want to know why they're afraid,\" Dunn said. \"You could ask the kids until they're blue in the face and you hardly ever find out why.\" Dunn meets with a half-dozen children once a week. She uses books, music and art to help the students overcome their fear of using the toilet. She helps them set small, realistic goals. For instance, she said, week one involved just sitting on the toilet for five minutes. She encourages positive reinforcement and simple rewards such as extra playtime with Mom or Dad. While Dunn works on the kids, psychologist Elaine Leclair, an instructor at the Harvard School of Medicine, offers frank advice to the parents in a separate room. \"I just say, 'Step back.' They hate to hear me say this, but I say whatever you're doing now is not working. You really need to try something different,\" Leclair said. She said many parents come to the sessions angry and anxious. \"They come in feeling extremely discouraged, very isolated thinking they are the only ones in the world who have this problem.\" Teres acknowledged that's how she felt. \"Imagine my surprise to find out there were thousands of kids who had this problem.\" After years of hiding her daughter's toilet training troubles from family and friends, the group parenting sessions allowed Teres to open up about her frustrations. \"It was like going to Betty Ford,\" joked Teres, who felt she had exhausted all her other options. A majority of the children who attend the Children's Hospital class are dealing with constipation issues often caused by delayed toilet training. Teres learned that her daughter had a medical condition called encopresis. Experts call it a symptom of chronic constipation and say it occurs when a child resists having a bowel movement. Youngsters like Molly are sometimes given laxatives or other medications to help encourage them to go. Schonwald, the author of \"The Pocket Idiot's Guide to Potty Training,\" doesn't want parents to be discouraged during the process. \"No one goes to college in diapers, right? Everyone will get through this time, as awful as it might feel if you're struggling.\" She suggested achieving success during potty training has to do with the approach. \"There are three things you cannot make a child do: eat, sleep or poop. We find that by decreasing the pressure and expectations that children feel more confident.\" Schonwald reminds parents that toilet training \"is not a chore, so keeping it positive from the very beginning is the most important thing.\" If a child isn't using the toilet by the age of 4, she recommends talking with a pediatrician. Schonwald also said setbacks during potty training are normal. \"All developmental skills come in spurts with periods of regression.\" After a setback, Schonwald advises parents to give their child positive messages that they can succeed by saying, \"So you had some accidents today. We'll try again tomorrow.\" Four months after graduating from \"poop school,\" Teres is relieved to report that Molly is fully toilet trained. \"You would never know this was an issue,\" said Teres. \"She even used an automatic flush toilet the other day. I waited to see her reaction and she was OK.\" E-mail to a friend . Judy Fortin is a correspondent with CNN Medical News. Linda Ciampa of Accent Health contributed to this report.</td>\n",
       "      <td>Most children show they're ready to potty train between 18 and 24 months .\\nMom resorted to hospital's potty school when daughter was in diapers at 4½ .\\nSix-week program uses creative methods to get kids comfortable with a toilet .\\nToilet training process can take more than a year, or longer for some kids .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   article  \\\n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          KARBALA, Iraq (CNN) -- A female suicide bomber apparently targeting Shiite worshippers killed at least 40 people and wounded at least 65 in Karbala on Monday, according to an Interior Ministry official. Iraqi security forces gather around the site of a car bomb explosion in Baghdad on Monday. The incident occurred one-half mile from the Imam Hussein shrine of Karbala. Karbala is a Shiite holy city, and the Imam Hussein shrine is one of Shiite Islam's holiest locations. The shrine marks the burial spot of Hussein bin Ali, the grandson of the Prophet Mohammed, who was killed in battle nearby in 680. No more information was immediately available about the blast southwest of the capital city, Baghdad. Earlier Monday, in Baghdad, a roadside bomb exploded near an Iraqi police patrol, killing one officer and wounding another, the Interior Ministry told CNN. A short time later, another roadside bomb exploded near an Iraqi police patrol on Palestine Street in eastern Baghdad, wounding four bystanders, a ministry official said. The first attack took place about 8:30 a.m. in the upscale Mansour neighborhood, where law enforcement officials have come under frequent attacks in recent weeks. Also Monday, two American soldiers were killed by a roadside bomb north of Baghdad, officials said. The incident occurred about 12:20 p.m.  as the soldiers were \"conducting a route-clearance combat operation north of Baghdad,\" according to a news release. The names of the soldiers were not immediately released. Meanwhile, U.S. Vice President Dick Cheney arrived in the Iraqi capital Monday on an unannounced visit. Cheney told reporters that the five years in Iraq since the war's start has been \"well worth the effort.\" He said he met with top Iraqi officials.  He appeared at a news conference with Gen. David Petraeus, the top U.S. commander in Iraq, and Ryan Crocker, the U.S. ambassador to the country. Cheney began a trip to the Middle East on Sunday with an official itinerary that listed stops in Oman, Saudi Arabia, Turkey, Israel and the West Bank, according to the White House. E-mail to a friend . CNN's Mohammed Tawfeeq contributed to this report.   \n",
       "7  BOSTON, Massachusetts (CNN) -- It all started with the flush of an automatic toilet. The terrifying sound marked the beginning of a two-year nightmare for Sarah Teres as she desperately tried to potty train her daughter Molly. Molly graduated from Potty School four days before her brother, Archer, was born. \"It was awful\" Teres said. \"We tried everything including bribery and threats.\" Teres, the mother of three from Andover, Massachusetts, hoped her middle child would be toilet trained by the time she was 2½. Two years later, the girl was still in diapers, refusing to use the bathroom. \"I was going crazy,\" Teres admitted. \"She wouldn't poop. She would hold it for days.\" At wits end, Teres enrolled Molly in the Toilet Training School at Children's Hospital Boston. \"By the time the children come in with their families, it has become a power struggle,\" explained Dr. Alison Schonwald, a pediatrician who supervises the \"poop school,\" as it's affectionately called by staffers. \"The kids kind of dig in their heels and put a line in the sand.\"  Health Minute: Watch more on the perils of potty training » . The American Academy of Pediatrics reports that most children show signs they're ready to potty train between 18 and 24 months. Doctors suggest a child may be ready to start trying if he's staying dry for at least two hours at a time during the day, walking to and from the bathroom, asking for a diaper change and asking to use the potty. For some kids, the toilet training process can take more than a year, or longer. The six-week program at Children's Hospital is one of a handful around the country. Kimberly Dunn, a pediatric nurse practitioner, has worked with some of the 450 young graduates over the years. She said most of the kids admit they are afraid to use a toilet. \"Oftentimes, the parents come in and they want to know why they're afraid,\" Dunn said. \"You could ask the kids until they're blue in the face and you hardly ever find out why.\" Dunn meets with a half-dozen children once a week. She uses books, music and art to help the students overcome their fear of using the toilet. She helps them set small, realistic goals. For instance, she said, week one involved just sitting on the toilet for five minutes. She encourages positive reinforcement and simple rewards such as extra playtime with Mom or Dad. While Dunn works on the kids, psychologist Elaine Leclair, an instructor at the Harvard School of Medicine, offers frank advice to the parents in a separate room. \"I just say, 'Step back.' They hate to hear me say this, but I say whatever you're doing now is not working. You really need to try something different,\" Leclair said. She said many parents come to the sessions angry and anxious. \"They come in feeling extremely discouraged, very isolated thinking they are the only ones in the world who have this problem.\" Teres acknowledged that's how she felt. \"Imagine my surprise to find out there were thousands of kids who had this problem.\" After years of hiding her daughter's toilet training troubles from family and friends, the group parenting sessions allowed Teres to open up about her frustrations. \"It was like going to Betty Ford,\" joked Teres, who felt she had exhausted all her other options. A majority of the children who attend the Children's Hospital class are dealing with constipation issues often caused by delayed toilet training. Teres learned that her daughter had a medical condition called encopresis. Experts call it a symptom of chronic constipation and say it occurs when a child resists having a bowel movement. Youngsters like Molly are sometimes given laxatives or other medications to help encourage them to go. Schonwald, the author of \"The Pocket Idiot's Guide to Potty Training,\" doesn't want parents to be discouraged during the process. \"No one goes to college in diapers, right? Everyone will get through this time, as awful as it might feel if you're struggling.\" She suggested achieving success during potty training has to do with the approach. \"There are three things you cannot make a child do: eat, sleep or poop. We find that by decreasing the pressure and expectations that children feel more confident.\" Schonwald reminds parents that toilet training \"is not a chore, so keeping it positive from the very beginning is the most important thing.\" If a child isn't using the toilet by the age of 4, she recommends talking with a pediatrician. Schonwald also said setbacks during potty training are normal. \"All developmental skills come in spurts with periods of regression.\" After a setback, Schonwald advises parents to give their child positive messages that they can succeed by saying, \"So you had some accidents today. We'll try again tomorrow.\" Four months after graduating from \"poop school,\" Teres is relieved to report that Molly is fully toilet trained. \"You would never know this was an issue,\" said Teres. \"She even used an automatic flush toilet the other day. I waited to see her reaction and she was OK.\" E-mail to a friend . Judy Fortin is a correspondent with CNN Medical News. Linda Ciampa of Accent Health contributed to this report.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                             highlights  \n",
       "6  NEW: 2 U.S. soldiers killed by roadside bomb in Baghdad on Monday, U.S. says .\\nNEW: Other roadside bombs in Baghdad kill one police officer, injure four people .\\nDeath toll rises to 40 in explosion in Karbala, official says; 65 injured .\\nExplosion was near holy shrine for Shiite Muslims, burial spot of Hussein bin Ali .  \n",
       "7                Most children show they're ready to potty train between 18 and 24 months .\\nMom resorted to hospital's potty school when daughter was in diapers at 4½ .\\nSix-week program uses creative methods to get kids comfortable with a toilet .\\nToilet training process can take more than a year, or longer for some kids .  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U2_3nE9PPC11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_data.shape: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are any Empty Cells in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZCCZf26-PDBA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IU72Gu1tPDNK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DovhJuAzP3Up"
   },
   "source": [
    "## Dealing with Duplicates / Drop Duplicates in the Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TLFV9-VNPDiK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "QvcTGPr8PDsX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rH72px5sPD2m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3mRd9-vZPEAI"
   },
   "outputs": [],
   "source": [
    "def del_duplicates(dataset, columns_to_compare):\n",
    "    '''\n",
    "    Function that deletes duplicated lines comapres according to indicated columns\n",
    "    '''\n",
    "    return dataset.drop_duplicates(subset=columns_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VvulFo2SQARj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['article', 'highlights']\n",
    "\n",
    "train = del_duplicates(train_data, columns_to_compare=cols)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ifg14tFdQAbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = del_duplicates(val_data, columns_to_compare=cols)\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K0I2DFGnQAlV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = del_duplicates(test_data, columns_to_compare=cols)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-obtjbnQMJn"
   },
   "source": [
    "## Potential threshhold for news extraction / Check the Statistical Distribution of Word Counts for Articles and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Wk1yYboZQA4N"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# word count\n",
    "article_wc = []\n",
    "summary_wc = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in train.article:\n",
    "      article_wc.append(len(i.split()))\n",
    "\n",
    "for i in train.highlights:\n",
    "      summary_wc.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':article_wc, 'summary':summary_wc})\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(length_df.text, bins=40)\n",
    "plt.xlabel(\"Word count\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(length_df.summary, bins=40)\n",
    "plt.xlabel(\"Word count\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OToRGc75QTef"
   },
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Function \"Preprocessing\" to perform the single cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform upper cases to lower cases\n",
    "- Remove return characters, url and html tags\n",
    "- Expand shortened words via Contractions\n",
    "- Remove any parentheses with text inside\n",
    "- Remove special characters, remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DaUHSqf9QBJ6"
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence: string, remove_stopwords=True) -> string: \n",
    "    \n",
    "    '''\n",
    "    Preprocessing text: lower case, \n",
    "                        deleting punctuation, \n",
    "                        replacing contructions with equivalent,\n",
    "                        deleting stop words,\n",
    "                        removing special characters\n",
    "    '''\n",
    "\n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "   \n",
    "    # Remove return characters, url and html tags\n",
    "    code_list = ['\\n', '\\S*(http|https)\\S*', '\\<a href', '&amp;', '<br />']\n",
    "    for code in code_list:\n",
    "        sentence = re.sub(code, ' ',sentence, flags=re.MULTILINE)\n",
    "    \n",
    "    # expand the shortened words (can't => can not)\n",
    "    # after they will be deleted in stopwords\n",
    "    expanded = []   \n",
    "    for word in sentence.split():\n",
    "        expanded.append(contractions.fix(word, slang=False))\n",
    "        \n",
    "    expanded_sentence = ' '.join(expanded)\n",
    "    \n",
    "    # remove any parenthisis with text inside\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', expanded_sentence)\n",
    "        # Removing punctuation, url and html tags\n",
    "    for punctuation in string.punctuation + '[\\'\\\"]':\n",
    "        sentence = sentence.replace(punctuation, ' ')\n",
    "        \n",
    "    # remove special characters \n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "\n",
    "    # Removing whitespaces\n",
    "    sentence = sentence.strip()\n",
    "                \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english')) ## defining stopwords    \n",
    "        sentence_list = [w for w in sentence.split() if not w in stop_words]\n",
    "        sentence = (' '.join(sentence_list)).strip()\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGDtjF4bQhjX"
   },
   "source": [
    "### Test the function \"Preprocessing\" prepared to clean the data on a subset to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gO80ix1OQWFi"
   },
   "outputs": [],
   "source": [
    "def cleaning(dataset: pd.Series, remove_stopwords=True) -> list:\n",
    "    '''\n",
    "    This function creates a cleaned version of each dataset.\n",
    "    Calls the preprocessing function.\n",
    "    '''\n",
    "    \n",
    "    clean = []\n",
    "    for text in dataset:\n",
    "        clean.append(preprocessing(text, remove_stopwords=True))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BKamJPHWQWPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 0 ns, total: 209 ms\n",
      "Wall time: 206 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = cleaning(train.article)\n",
    "y_train = cleaning(train.highlights, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RmrmvZSZQWXn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean article : football superstar celebrity fashion icon multimillion dollar heartthrob david beckham headed hollywood hills takes game u major league soccer cnn looks bekham fulfilled dream playing manchester united time playing england world famous footballer begun five year contract los angeles galaxy team friday beckham meet press reveal new shirt number week take depth look life times beckham cnn becks becky anderson sets examine makes man tick footballer fashion icon global phenomenon long way streets east london hollywood hills becky charts beckham incredible rise football stardom journey seen skills grace greatest stages world soccer goes pursuit current hottest property sports celebrity circuit u along way explores exactly behind man golden boot cnn look back life beckham wonderfully talented youngster fulfilled dream playing manchester united marriage pop star victoria trials tribulations playing england look highs lows man u departure galacticos madrid home depot stadium l ask beckham family adapt life los angeles people places see seen celebrity endorsement beckham stranger exposure teamed reggie bush adidas commercial face motorola face playstation game need fashion tips international clothing line star couple need become accepted part tinseltown glitterati road major league football u well worn route world greatest players talk former greats came examine impact overseas stars u soccer look different also get rare glimpse inside david beckham academy l find drives kids heroes perception u soccer game girls teenage years changing young kids choosing european game traditional u sports e mail friend\n",
      "\n",
      "\n",
      "Clean summary : beckham agreed five year contract los angeles galaxy new contract took effect july former english captain meet press unveil new shirt number friday cnn look beckham footballer fashion icon global phenomenon\n"
     ]
    }
   ],
   "source": [
    "print(f'Clean article : {X_train[0]}')\n",
    "print('\\n')\n",
    "print(f'Clean summary : {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BFEXlGzDQWg7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 294 ms, sys: 0 ns, total: 294 ms\n",
      "Wall time: 291 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_val = cleaning(val.article)\n",
    "y_val = cleaning(val.highlights, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ps5m8v4OQpdY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 229 ms, sys: 0 ns, total: 229 ms\n",
      "Wall time: 225 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_test = cleaning(test.highlights, remove_stopwords=False)\n",
    "y_test = cleaning(test.article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85bGfFjNQxhV"
   },
   "source": [
    "### !!! Only for target data => adding \"start\" and \"stop\" to the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "s5W6W_maQpvz"
   },
   "outputs": [],
   "source": [
    "def adding_decoder_tokens(data: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Adding special tokens for the decoder only to target string\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(data).apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "y_train = adding_decoder_tokens(y_train)\n",
    "y_val = adding_decoder_tokens(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "H7Z0Y7kQQp4G"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_START_ beckham agreed five year contract los angeles galaxy new contract took effect july former english captain meet press unveil new shirt number friday cnn look beckham footballer fashion icon global phenomenon _END_'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYq42bubQ63S"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the max len for Article and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yl7bnjFEQ4BO"
   },
   "outputs": [],
   "source": [
    "# initialize the max len for article and summary\n",
    "max_len_text = 150\n",
    "max_len_summary= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1YDlcmnRAjB"
   },
   "source": [
    "### Transform each Article in articles to a sequence of Integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this step are the lists X_train_tok and X_val_tok that basically are lists of tokenized articles in turn being lists of words transformed to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sGkx77RsQ4WP"
   },
   "outputs": [],
   "source": [
    "# learning the dictionnary from train articles\n",
    "X_tokenizer = Tokenizer()\n",
    "X_tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# Transforms each article in articles to a sequence of integers.\n",
    "X_train_tok = X_tokenizer.texts_to_sequences(X_train) \n",
    "X_val_tok = X_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "P8GHEt9MQ4f-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized article looks like this : \n",
      "[273, 4126, 960, 1141, 2072, 4127, 2073, 4128, 405, 831, 961, 447, 1142, 1143, 196, 9, 233, 729, 1642, 6, 1643, 4129, 2786, 1644, 448, 2787, 57, 10, 448, 367, 32, 1359, 2074, 1645, 99, 12, 1360, 406, 449, 4130, 135, 41, 831, 1144, 368, 2788, 5, 4131, 165, 74, 54, 4132, 450, 94, 274, 831, 6, 4133, 2075, 832, 1145, 2076, 730, 106, 4134, 2074, 1141, 2072, 833, 2789, 100, 78, 576, 651, 155, 447, 1142, 2075, 4135, 831, 2077, 2790, 273, 4136, 2078, 214, 2791, 2079, 834, 2792, 32, 1642, 501, 4137, 731, 4138, 1361, 732, 960, 2793, 9, 252, 78, 4139, 962, 329, 106, 2080, 4140, 6, 450, 29, 94, 831, 2794, 2795, 4141, 2786, 1644, 448, 2787, 57, 963, 2796, 330, 2797, 2798, 4142, 448, 367, 450, 4143, 4144, 106, 9, 2799, 4145, 964, 43, 4146, 502, 1646, 835, 831, 58, 4147, 94, 406, 449, 2, 1146, 51, 214, 960, 4148, 831, 2081, 2800, 4149, 2801, 369, 4150, 1647, 234, 4151, 234, 4152, 196, 112, 1141, 4153, 125, 4154, 836, 330, 503, 112, 407, 4155, 47, 4156, 4157, 837, 233, 729, 273, 9, 31, 2082, 652, 32, 834, 965, 294, 104, 2802, 136, 2076, 966, 2083, 2803, 9, 1642, 450, 451, 7, 20, 1362, 4158, 733, 405, 831, 1363, 1646, 166, 1648, 577, 4159, 4160, 9, 1642, 196, 2804, 2805, 16, 1364, 253, 577, 2084, 1147, 196, 1365, 9, 732, 331, 504, 452]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized article looks like this : ')\n",
    "print(X_train_tok[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the Tokenized Articles to max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The padding step shall ensure that all tokenized articles are split in a way that the results are lists of integers all having the same length max_len_text\n",
    "- If a tokenized article is longer than max_len_text it is truncated\n",
    "- If a tokenized article is shorter than max_len_text it is filled with 0's to reach a length of max_len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XaRpUF6UQWpO"
   },
   "outputs": [],
   "source": [
    "# post-padding with zeros up to maximum length\n",
    "X_train_pad = pad_sequences(X_train_tok, dtype='float32', maxlen=max_len_text, padding='post') \n",
    "X_val_pad = pad_sequences(X_val_tok, dtype='float32', maxlen=max_len_text, padding='post')\n",
    "\n",
    "X_vocab = len(X_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0uj0QmuwRHCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train articles vocabulary is 8124\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of train articles vocabulary is {X_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbWq5nCdRbhq"
   },
   "source": [
    "## Summary Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Summaries to lists of Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "d8L4ZtTpRHSm"
   },
   "outputs": [],
   "source": [
    "# learning the dictionnary from train summaries\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# Transforms each summary in summaries to a sequence of integers.\n",
    "y_train_tok = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val_tok = y_tokenizer.texts_to_sequences(y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the tokenized Summaries to a Length of max_len_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "60Evu6O4RHeV"
   },
   "outputs": [],
   "source": [
    "# post-padding with zeros up to maximum length\n",
    "y_train_pad = pad_sequences(y_train_tok, dtype='float32', maxlen=max_len_summary, padding='post')\n",
    "y_val_pad = pad_sequences(y_val_tok,  dtype='float32', maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_vocab = len(y_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KLKOzFNKRH_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train summary vocabulary is 1802\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of train summary vocabulary is {y_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  28.],\n",
       "        [  29.],\n",
       "        [ 106.],\n",
       "        [ 229.],\n",
       "        [ 525.],\n",
       "        [ 526.],\n",
       "        [ 527.],\n",
       "        [ 528.],\n",
       "        [ 529.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 535.],\n",
       "        [ 536.],\n",
       "        [ 112.],\n",
       "        [ 110.],\n",
       "        [  62.],\n",
       "        [ 107.],\n",
       "        [ 113.],\n",
       "        [ 537.],\n",
       "        [ 538.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 547.],\n",
       "        [ 548.],\n",
       "        [ 549.],\n",
       "        [ 550.],\n",
       "        [ 551.],\n",
       "        [ 235.],\n",
       "        [ 552.],\n",
       "        [ 553.],\n",
       "        [ 115.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 562.],\n",
       "        [ 240.],\n",
       "        [ 117.],\n",
       "        [ 563.],\n",
       "        [ 564.],\n",
       "        [ 118.],\n",
       "        [ 119.],\n",
       "        [ 565.],\n",
       "        [ 120.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 577.],\n",
       "        [ 244.],\n",
       "        [  10.],\n",
       "        [ 245.],\n",
       "        [ 246.],\n",
       "        [ 578.],\n",
       "        [ 247.],\n",
       "        [ 248.],\n",
       "        [ 579.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 587.],\n",
       "        [ 588.],\n",
       "        [ 251.],\n",
       "        [ 252.],\n",
       "        [ 589.],\n",
       "        [ 590.],\n",
       "        [ 123.],\n",
       "        [ 591.],\n",
       "        [ 253.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 598.],\n",
       "        [ 599.],\n",
       "        [ 600.],\n",
       "        [ 601.],\n",
       "        [ 602.],\n",
       "        [ 603.],\n",
       "        [ 604.],\n",
       "        [ 605.],\n",
       "        [ 606.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 615.],\n",
       "        [ 260.],\n",
       "        [ 260.],\n",
       "        [ 134.],\n",
       "        [ 261.],\n",
       "        [ 108.],\n",
       "        [  13.],\n",
       "        [ 616.],\n",
       "        [ 133.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 630.],\n",
       "        [ 631.],\n",
       "        [ 140.],\n",
       "        [ 632.],\n",
       "        [  70.],\n",
       "        [  71.],\n",
       "        [ 633.],\n",
       "        [ 634.],\n",
       "        [ 141.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 646.],\n",
       "        [ 647.],\n",
       "        [ 271.],\n",
       "        [ 272.],\n",
       "        [  73.],\n",
       "        [ 135.],\n",
       "        [ 273.],\n",
       "        [   8.],\n",
       "        [ 648.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 653.],\n",
       "        [ 144.],\n",
       "        [ 654.],\n",
       "        [ 276.],\n",
       "        [ 655.],\n",
       "        [ 656.],\n",
       "        [ 657.],\n",
       "        [ 658.],\n",
       "        [  34.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 281.],\n",
       "        [ 278.],\n",
       "        [ 274.],\n",
       "        [ 673.],\n",
       "        [ 674.],\n",
       "        [ 675.],\n",
       "        [ 676.],\n",
       "        [ 113.],\n",
       "        [ 149.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 153.],\n",
       "        [ 687.],\n",
       "        [  62.],\n",
       "        [ 284.],\n",
       "        [ 285.],\n",
       "        [  23.],\n",
       "        [ 290.],\n",
       "        [ 688.],\n",
       "        [ 689.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 699.],\n",
       "        [ 292.],\n",
       "        [ 700.],\n",
       "        [ 701.],\n",
       "        [ 291.],\n",
       "        [  72.],\n",
       "        [ 702.],\n",
       "        [ 120.],\n",
       "        [ 118.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 294.],\n",
       "        [ 154.],\n",
       "        [ 708.],\n",
       "        [   5.],\n",
       "        [   9.],\n",
       "        [ 709.],\n",
       "        [ 710.],\n",
       "        [  17.],\n",
       "        [ 155.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 157.],\n",
       "        [ 158.],\n",
       "        [  11.],\n",
       "        [ 720.],\n",
       "        [ 721.],\n",
       "        [ 722.],\n",
       "        [ 159.],\n",
       "        [ 295.],\n",
       "        [ 159.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  37.],\n",
       "        [ 737.],\n",
       "        [ 738.],\n",
       "        [  11.],\n",
       "        [ 161.],\n",
       "        [ 739.],\n",
       "        [ 740.],\n",
       "        [ 741.],\n",
       "        [ 742.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 764.],\n",
       "        [ 163.],\n",
       "        [ 300.],\n",
       "        [ 303.],\n",
       "        [ 106.],\n",
       "        [  78.],\n",
       "        [ 765.],\n",
       "        [ 766.],\n",
       "        [ 767.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  48.],\n",
       "        [  24.],\n",
       "        [  62.],\n",
       "        [ 779.],\n",
       "        [ 309.],\n",
       "        [ 310.],\n",
       "        [ 780.],\n",
       "        [ 781.],\n",
       "        [ 311.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 797.],\n",
       "        [ 798.],\n",
       "        [  14.],\n",
       "        [ 799.],\n",
       "        [ 800.],\n",
       "        [ 801.],\n",
       "        [ 802.],\n",
       "        [ 314.],\n",
       "        [ 315.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 816.],\n",
       "        [ 817.],\n",
       "        [ 818.],\n",
       "        [ 819.],\n",
       "        [ 820.],\n",
       "        [ 821.],\n",
       "        [ 822.],\n",
       "        [ 823.],\n",
       "        [ 824.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 322.],\n",
       "        [ 833.],\n",
       "        [ 834.],\n",
       "        [ 835.],\n",
       "        [ 836.],\n",
       "        [ 837.],\n",
       "        [ 838.],\n",
       "        [ 325.],\n",
       "        [ 839.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 852.],\n",
       "        [ 853.],\n",
       "        [  51.],\n",
       "        [ 854.],\n",
       "        [  38.],\n",
       "        [ 855.],\n",
       "        [ 856.],\n",
       "        [ 857.],\n",
       "        [ 858.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 871.],\n",
       "        [  18.],\n",
       "        [ 116.],\n",
       "        [ 872.],\n",
       "        [  49.],\n",
       "        [ 283.],\n",
       "        [ 873.],\n",
       "        [ 303.],\n",
       "        [ 874.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 888.],\n",
       "        [  84.],\n",
       "        [ 889.],\n",
       "        [  83.],\n",
       "        [ 890.],\n",
       "        [ 891.],\n",
       "        [   3.],\n",
       "        [ 339.],\n",
       "        [ 892.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 897.],\n",
       "        [ 170.],\n",
       "        [ 898.],\n",
       "        [ 281.],\n",
       "        [  24.],\n",
       "        [  85.],\n",
       "        [ 343.],\n",
       "        [ 899.],\n",
       "        [ 127.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  37.],\n",
       "        [ 909.],\n",
       "        [ 910.],\n",
       "        [ 911.],\n",
       "        [ 319.],\n",
       "        [ 346.],\n",
       "        [ 347.],\n",
       "        [ 912.],\n",
       "        [  15.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 918.],\n",
       "        [ 349.],\n",
       "        [ 919.],\n",
       "        [ 920.],\n",
       "        [   4.],\n",
       "        [ 921.],\n",
       "        [ 351.],\n",
       "        [ 922.],\n",
       "        [ 923.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 929.],\n",
       "        [ 140.],\n",
       "        [ 353.],\n",
       "        [  46.],\n",
       "        [ 930.],\n",
       "        [ 153.],\n",
       "        [  64.],\n",
       "        [   7.],\n",
       "        [ 931.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 355.],\n",
       "        [ 943.],\n",
       "        [ 175.],\n",
       "        [ 944.],\n",
       "        [ 945.],\n",
       "        [ 946.],\n",
       "        [ 358.],\n",
       "        [  10.],\n",
       "        [ 947.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 957.],\n",
       "        [ 177.],\n",
       "        [ 958.],\n",
       "        [ 959.],\n",
       "        [ 359.],\n",
       "        [ 960.],\n",
       "        [ 961.],\n",
       "        [  43.],\n",
       "        [ 362.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 969.],\n",
       "        [ 970.],\n",
       "        [   3.],\n",
       "        [  11.],\n",
       "        [ 971.],\n",
       "        [ 363.],\n",
       "        [ 364.],\n",
       "        [ 972.],\n",
       "        [ 178.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 372.],\n",
       "        [ 253.],\n",
       "        [ 983.],\n",
       "        [  11.],\n",
       "        [  12.],\n",
       "        [ 372.],\n",
       "        [ 984.],\n",
       "        [ 985.],\n",
       "        [  88.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 373.],\n",
       "        [ 991.],\n",
       "        [ 275.],\n",
       "        [  33.],\n",
       "        [ 992.],\n",
       "        [  31.],\n",
       "        [  15.],\n",
       "        [ 134.],\n",
       "        [  73.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1003.],\n",
       "        [   5.],\n",
       "        [   9.],\n",
       "        [  36.],\n",
       "        [1004.],\n",
       "        [1005.],\n",
       "        [1006.],\n",
       "        [ 308.],\n",
       "        [ 346.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1021.],\n",
       "        [ 164.],\n",
       "        [ 122.],\n",
       "        [ 376.],\n",
       "        [ 379.],\n",
       "        [  90.],\n",
       "        [  19.],\n",
       "        [  35.],\n",
       "        [ 380.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1036.],\n",
       "        [1037.],\n",
       "        [ 187.],\n",
       "        [1038.],\n",
       "        [  11.],\n",
       "        [1039.],\n",
       "        [  40.],\n",
       "        [1040.],\n",
       "        [1041.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  67.],\n",
       "        [1050.],\n",
       "        [ 384.],\n",
       "        [ 385.],\n",
       "        [  91.],\n",
       "        [ 384.],\n",
       "        [ 189.],\n",
       "        [  41.],\n",
       "        [ 386.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1061.],\n",
       "        [  33.],\n",
       "        [1062.],\n",
       "        [  48.],\n",
       "        [  80.],\n",
       "        [1063.],\n",
       "        [1064.],\n",
       "        [1065.],\n",
       "        [1066.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  94.],\n",
       "        [ 192.],\n",
       "        [1075.],\n",
       "        [1076.],\n",
       "        [  11.],\n",
       "        [1077.],\n",
       "        [1078.],\n",
       "        [1079.],\n",
       "        [ 392.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1088.],\n",
       "        [ 396.],\n",
       "        [1089.],\n",
       "        [1090.],\n",
       "        [1091.],\n",
       "        [1092.],\n",
       "        [1093.],\n",
       "        [1094.],\n",
       "        [ 395.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1098.],\n",
       "        [1099.],\n",
       "        [   6.],\n",
       "        [ 193.],\n",
       "        [  55.],\n",
       "        [1100.],\n",
       "        [ 397.],\n",
       "        [1101.],\n",
       "        [1102.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1109.],\n",
       "        [1110.],\n",
       "        [1111.],\n",
       "        [ 401.],\n",
       "        [ 401.],\n",
       "        [  72.],\n",
       "        [ 402.],\n",
       "        [  32.],\n",
       "        [ 399.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 301.],\n",
       "        [ 404.],\n",
       "        [   3.],\n",
       "        [1122.],\n",
       "        [1123.],\n",
       "        [   6.],\n",
       "        [1124.],\n",
       "        [1125.],\n",
       "        [1126.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 390.],\n",
       "        [ 357.],\n",
       "        [  81.],\n",
       "        [ 406.],\n",
       "        [1134.],\n",
       "        [ 198.],\n",
       "        [  17.],\n",
       "        [1135.],\n",
       "        [1136.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1148.],\n",
       "        [ 409.],\n",
       "        [1149.],\n",
       "        [1150.],\n",
       "        [ 407.],\n",
       "        [  89.],\n",
       "        [1151.],\n",
       "        [  30.],\n",
       "        [   3.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1158.],\n",
       "        [1159.],\n",
       "        [   3.],\n",
       "        [  38.],\n",
       "        [ 412.],\n",
       "        [1160.],\n",
       "        [1161.],\n",
       "        [1162.],\n",
       "        [1163.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 111.],\n",
       "        [ 201.],\n",
       "        [1173.],\n",
       "        [1174.],\n",
       "        [1175.],\n",
       "        [ 317.],\n",
       "        [1176.],\n",
       "        [  18.],\n",
       "        [ 232.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1185.],\n",
       "        [ 202.],\n",
       "        [1186.],\n",
       "        [1187.],\n",
       "        [ 421.],\n",
       "        [ 169.],\n",
       "        [  35.],\n",
       "        [1188.],\n",
       "        [ 422.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  18.],\n",
       "        [  65.],\n",
       "        [1196.],\n",
       "        [  26.],\n",
       "        [1197.],\n",
       "        [1198.],\n",
       "        [1199.],\n",
       "        [1200.],\n",
       "        [ 110.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1211.],\n",
       "        [1212.],\n",
       "        [1213.],\n",
       "        [  72.],\n",
       "        [  97.],\n",
       "        [  12.],\n",
       "        [ 280.],\n",
       "        [1214.],\n",
       "        [ 429.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 430.],\n",
       "        [1227.],\n",
       "        [1228.],\n",
       "        [1229.],\n",
       "        [1230.],\n",
       "        [1231.],\n",
       "        [1232.],\n",
       "        [1233.],\n",
       "        [ 252.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 431.],\n",
       "        [1244.],\n",
       "        [1245.],\n",
       "        [  56.],\n",
       "        [1246.],\n",
       "        [1247.],\n",
       "        [1248.],\n",
       "        [ 433.],\n",
       "        [ 104.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 206.],\n",
       "        [ 434.],\n",
       "        [1259.],\n",
       "        [   4.],\n",
       "        [1260.],\n",
       "        [1261.],\n",
       "        [ 435.],\n",
       "        [ 152.],\n",
       "        [   3.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1273.],\n",
       "        [1274.],\n",
       "        [ 437.],\n",
       "        [ 207.],\n",
       "        [ 173.],\n",
       "        [ 230.],\n",
       "        [ 438.],\n",
       "        [1275.],\n",
       "        [1276.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1287.],\n",
       "        [ 209.],\n",
       "        [1288.],\n",
       "        [ 231.],\n",
       "        [1289.],\n",
       "        [1290.],\n",
       "        [1291.],\n",
       "        [1292.],\n",
       "        [1293.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 328.],\n",
       "        [1308.],\n",
       "        [  30.],\n",
       "        [  75.],\n",
       "        [ 305.],\n",
       "        [1309.],\n",
       "        [ 210.],\n",
       "        [ 441.],\n",
       "        [ 442.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 444.],\n",
       "        [ 272.],\n",
       "        [1317.],\n",
       "        [1318.],\n",
       "        [1319.],\n",
       "        [1320.],\n",
       "        [1321.],\n",
       "        [ 443.],\n",
       "        [   3.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 447.],\n",
       "        [ 445.],\n",
       "        [   3.],\n",
       "        [  73.],\n",
       "        [ 158.],\n",
       "        [ 448.],\n",
       "        [ 210.],\n",
       "        [1332.],\n",
       "        [ 261.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1339.],\n",
       "        [1340.],\n",
       "        [ 452.],\n",
       "        [1341.],\n",
       "        [1342.],\n",
       "        [1343.],\n",
       "        [ 212.],\n",
       "        [ 198.],\n",
       "        [1344.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1354.],\n",
       "        [ 214.],\n",
       "        [  43.],\n",
       "        [   6.],\n",
       "        [1355.],\n",
       "        [1356.],\n",
       "        [1357.],\n",
       "        [   8.],\n",
       "        [1358.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1366.],\n",
       "        [ 236.],\n",
       "        [1367.],\n",
       "        [ 325.],\n",
       "        [1368.],\n",
       "        [1369.],\n",
       "        [1370.],\n",
       "        [   5.],\n",
       "        [   9.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 454.],\n",
       "        [  68.],\n",
       "        [ 437.],\n",
       "        [   3.],\n",
       "        [1380.],\n",
       "        [ 455.],\n",
       "        [1381.],\n",
       "        [1382.],\n",
       "        [1383.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1394.],\n",
       "        [1395.],\n",
       "        [ 459.],\n",
       "        [  24.],\n",
       "        [ 460.],\n",
       "        [  47.],\n",
       "        [ 126.],\n",
       "        [  47.],\n",
       "        [ 217.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 463.],\n",
       "        [  20.],\n",
       "        [  13.],\n",
       "        [  99.],\n",
       "        [  10.],\n",
       "        [1405.],\n",
       "        [1406.],\n",
       "        [ 462.],\n",
       "        [  53.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 191.],\n",
       "        [1410.],\n",
       "        [1411.],\n",
       "        [1412.],\n",
       "        [  59.],\n",
       "        [1413.],\n",
       "        [1414.],\n",
       "        [ 176.],\n",
       "        [ 464.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1421.],\n",
       "        [  77.],\n",
       "        [ 309.],\n",
       "        [1422.],\n",
       "        [ 287.],\n",
       "        [1423.],\n",
       "        [ 465.],\n",
       "        [  55.],\n",
       "        [  93.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 205.],\n",
       "        [1428.],\n",
       "        [ 147.],\n",
       "        [1429.],\n",
       "        [1430.],\n",
       "        [1431.],\n",
       "        [1432.],\n",
       "        [ 118.],\n",
       "        [1433.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  57.],\n",
       "        [  61.],\n",
       "        [  34.],\n",
       "        [  57.],\n",
       "        [ 219.],\n",
       "        [1443.],\n",
       "        [1444.],\n",
       "        [ 265.],\n",
       "        [  13.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1451.],\n",
       "        [1452.],\n",
       "        [ 392.],\n",
       "        [ 100.],\n",
       "        [  40.],\n",
       "        [ 102.],\n",
       "        [1453.],\n",
       "        [  39.],\n",
       "        [1454.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 196.],\n",
       "        [ 187.],\n",
       "        [ 112.],\n",
       "        [ 124.],\n",
       "        [1464.],\n",
       "        [  94.],\n",
       "        [1465.],\n",
       "        [ 473.],\n",
       "        [ 383.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1478.],\n",
       "        [1479.],\n",
       "        [1480.],\n",
       "        [  23.],\n",
       "        [ 420.],\n",
       "        [1481.],\n",
       "        [1482.],\n",
       "        [ 477.],\n",
       "        [  41.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 293.],\n",
       "        [  66.],\n",
       "        [  95.],\n",
       "        [1494.],\n",
       "        [1495.],\n",
       "        [1496.],\n",
       "        [ 213.],\n",
       "        [  92.],\n",
       "        [1497.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1511.],\n",
       "        [1512.],\n",
       "        [  24.],\n",
       "        [1513.],\n",
       "        [ 143.],\n",
       "        [1514.],\n",
       "        [1515.],\n",
       "        [1516.],\n",
       "        [1517.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1522.],\n",
       "        [ 215.],\n",
       "        [  55.],\n",
       "        [1523.],\n",
       "        [  68.],\n",
       "        [1524.],\n",
       "        [1525.],\n",
       "        [1526.],\n",
       "        [ 480.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1537.],\n",
       "        [1538.],\n",
       "        [ 461.],\n",
       "        [1539.],\n",
       "        [1540.],\n",
       "        [ 223.],\n",
       "        [  98.],\n",
       "        [ 140.],\n",
       "        [1541.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1551.],\n",
       "        [  34.],\n",
       "        [1552.],\n",
       "        [1553.],\n",
       "        [1554.],\n",
       "        [  28.],\n",
       "        [1555.],\n",
       "        [1556.],\n",
       "        [  45.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1560.],\n",
       "        [1561.],\n",
       "        [1562.],\n",
       "        [  15.],\n",
       "        [  24.],\n",
       "        [ 102.],\n",
       "        [1563.],\n",
       "        [  22.],\n",
       "        [  53.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  32.],\n",
       "        [1570.],\n",
       "        [  67.],\n",
       "        [ 114.],\n",
       "        [   3.],\n",
       "        [ 146.],\n",
       "        [1571.],\n",
       "        [ 485.],\n",
       "        [1572.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1582.],\n",
       "        [1583.],\n",
       "        [1584.],\n",
       "        [1585.],\n",
       "        [1586.],\n",
       "        [   7.],\n",
       "        [  82.],\n",
       "        [1587.],\n",
       "        [   3.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 342.],\n",
       "        [1593.],\n",
       "        [1594.],\n",
       "        [ 491.],\n",
       "        [  12.],\n",
       "        [1595.],\n",
       "        [  58.],\n",
       "        [1596.],\n",
       "        [  19.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1610.],\n",
       "        [ 494.],\n",
       "        [  23.],\n",
       "        [ 493.],\n",
       "        [ 492.],\n",
       "        [1611.],\n",
       "        [1612.],\n",
       "        [1613.],\n",
       "        [ 206.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 413.],\n",
       "        [1630.],\n",
       "        [1631.],\n",
       "        [ 122.],\n",
       "        [1632.],\n",
       "        [1633.],\n",
       "        [  96.],\n",
       "        [1634.],\n",
       "        [1635.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 226.],\n",
       "        [ 225.],\n",
       "        [ 428.],\n",
       "        [  32.],\n",
       "        [ 454.],\n",
       "        [1643.],\n",
       "        [ 367.],\n",
       "        [ 223.],\n",
       "        [ 495.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  54.],\n",
       "        [1648.],\n",
       "        [1649.],\n",
       "        [1650.],\n",
       "        [1651.],\n",
       "        [ 201.],\n",
       "        [1652.],\n",
       "        [  33.],\n",
       "        [1653.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  39.],\n",
       "        [1657.],\n",
       "        [  83.],\n",
       "        [   3.],\n",
       "        [  53.],\n",
       "        [ 307.],\n",
       "        [  39.],\n",
       "        [ 448.],\n",
       "        [   3.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 262.],\n",
       "        [1665.],\n",
       "        [1666.],\n",
       "        [ 264.],\n",
       "        [ 498.],\n",
       "        [ 101.],\n",
       "        [  20.],\n",
       "        [  44.],\n",
       "        [  15.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 500.],\n",
       "        [ 501.],\n",
       "        [  23.],\n",
       "        [ 179.],\n",
       "        [ 502.],\n",
       "        [ 503.],\n",
       "        [ 504.],\n",
       "        [  52.],\n",
       "        [ 162.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 237.],\n",
       "        [  29.],\n",
       "        [ 506.],\n",
       "        [  16.],\n",
       "        [ 124.],\n",
       "        [  85.],\n",
       "        [1682.],\n",
       "        [ 193.],\n",
       "        [1683.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 273.],\n",
       "        [1696.],\n",
       "        [1697.],\n",
       "        [ 505.],\n",
       "        [1698.],\n",
       "        [1699.],\n",
       "        [  50.],\n",
       "        [1700.],\n",
       "        [ 155.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[  19.],\n",
       "        [  26.],\n",
       "        [  38.],\n",
       "        [1704.],\n",
       "        [  39.],\n",
       "        [1705.],\n",
       "        [1706.],\n",
       "        [  26.],\n",
       "        [1707.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 228.],\n",
       "        [ 509.],\n",
       "        [1713.],\n",
       "        [1714.],\n",
       "        [  84.],\n",
       "        [ 330.],\n",
       "        [ 215.],\n",
       "        [  20.],\n",
       "        [ 463.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1721.],\n",
       "        [1722.],\n",
       "        [1723.],\n",
       "        [  78.],\n",
       "        [ 218.],\n",
       "        [1724.],\n",
       "        [1725.],\n",
       "        [  52.],\n",
       "        [1726.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1737.],\n",
       "        [1738.],\n",
       "        [1739.],\n",
       "        [1740.],\n",
       "        [ 145.],\n",
       "        [  12.],\n",
       "        [  95.],\n",
       "        [1741.],\n",
       "        [1742.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 512.],\n",
       "        [1750.],\n",
       "        [1751.],\n",
       "        [  86.],\n",
       "        [1752.],\n",
       "        [ 156.],\n",
       "        [ 510.],\n",
       "        [ 511.],\n",
       "        [1753.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1758.],\n",
       "        [ 113.],\n",
       "        [ 227.],\n",
       "        [1759.],\n",
       "        [  93.],\n",
       "        [  25.],\n",
       "        [1760.],\n",
       "        [ 212.],\n",
       "        [1761.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1769.],\n",
       "        [ 486.],\n",
       "        [  10.],\n",
       "        [   3.],\n",
       "        [  51.],\n",
       "        [1770.],\n",
       "        [1771.],\n",
       "        [1772.],\n",
       "        [ 396.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[ 500.],\n",
       "        [ 501.],\n",
       "        [  23.],\n",
       "        [ 179.],\n",
       "        [ 502.],\n",
       "        [ 503.],\n",
       "        [ 504.],\n",
       "        [  52.],\n",
       "        [ 162.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1790.],\n",
       "        [ 126.],\n",
       "        [ 221.],\n",
       "        [ 441.],\n",
       "        [1791.],\n",
       "        [ 211.],\n",
       "        [  62.],\n",
       "        [ 336.],\n",
       "        [1792.],\n",
       "        [   2.]],\n",
       "\n",
       "       [[1796.],\n",
       "        [  45.],\n",
       "        [1797.],\n",
       "        [ 476.],\n",
       "        [1798.],\n",
       "        [1799.],\n",
       "        [1800.],\n",
       "        [1801.],\n",
       "        [ 427.],\n",
       "        [   2.]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model that will be Trained and will Predict Summaries of Articles based on the Training Performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules Potentially Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5BHCPHk3Zfq_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define latent_dim & embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- latent_dim is the dimension intended for the output of the Encoder, i.e. the first step of the Model\n",
    "- embedding_dim is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "k8-MdhYBo9oC"
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmpIHCQqRxdk"
   },
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jzYbuaHh0XH"
   },
   "source": [
    "## Step 1 of the Model: Embed the Inputs to the Encoder into the Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Encoder allocates the integers (being included in the tokenized and padded lists of integers) inserted into the Model into Vectors in the \"Latent Space\"\n",
    "- Input shape is max_len_text as via the step before we have padded the lists of integers to a length of max_len_text\n",
    "- X_Vocab is the length of the X_Tokenizer Word Index + 1, i.e. the number of integers + 1. Why \"+1\"? xxx\n",
    "- latent_dim is the dimensionality of the output space\n",
    "- The result of the Encoder, \"enc_emb\", is a Tensor having the shape (None, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "t7Zf8IJdhlpc"
   },
   "outputs": [],
   "source": [
    "# Add documentation for encoder\n",
    "# shape: max_len_text, i.e. the maximum length of words of the input text we will insert into the model, correct?\n",
    "# Embedding: X_vocab is the length of the X_Tokenizer Word Index + 1\n",
    "# Embedding: latent_dim is the dimensionality of the output space\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(X_vocab, latent_dim,trainable=True)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ9TKv9zib6y"
   },
   "source": [
    "## Step 2 of the Model: Three Stages of Stacked Long Short Term Memory (LSTM) Acting as the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three LSTM layers\n",
    "\n",
    "The outputs of this step are\n",
    "\n",
    "- encoder_outputs being a tensor with shape (None, max_len_text, max_len_text)\n",
    "- state_h being a tensor with shape (none, max_len_text)\n",
    "- state_c being a tensor with shape (none, max_len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRFKSPDjiaEI"
   },
   "outputs": [],
   "source": [
    "# LSTM 1 \n",
    "# first integer shown in the brackets is the \"dimensionality of the output space\". so, that would be the length of the output summary, right?\n",
    "# return_sequences = Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "# return_state = Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDWYA9T-is-v"
   },
   "source": [
    "## Step 3 of the Model: Setup the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoYbT4rAia4x"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_vocab, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsTzUy0cjgAN"
   },
   "source": [
    "Decoder based on encoder_states as initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBx3LRh5jkeV"
   },
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNipeqwZj5lB"
   },
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUd41C8_jlMc"
   },
   "outputs": [],
   "source": [
    "#attn_layer = Attention(name='attention_layer') \n",
    "#attn_out = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "attention = Attention(name='attention_layer')\n",
    "attn_out = attention([decoder_outputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQuvnGixkDfM"
   },
   "source": [
    "Concatenate attention output and decoder LSTM output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCjlmO-VkGuW"
   },
   "outputs": [],
   "source": [
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_concat_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M93_0V5TkbkJ"
   },
   "source": [
    "Add a Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xu59RA5pkHDh"
   },
   "outputs": [],
   "source": [
    "#decoder_dense = Dense(y_vocab, activation='softmax')\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab, activation='softmax')) \n",
    "#decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srb_Kzsikj1S"
   },
   "source": [
    "Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXFVF-5xkHX7"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = optimizers.RMSprop(learning_rate=0.00001)\n",
    "model.compile( \n",
    "    #optimizer = opt, loss=\"sparse_categorical_crossentropy\")\n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\") \n",
    "history = model.fit( \n",
    "    [X_train_pad,y_train_pad[:,:-1]],\n",
    "    y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)[:,1:], \n",
    "    batch_size=30, \n",
    "    epochs=10,\n",
    "    #callbacks=[es],\n",
    "    validation_data=([X_val_pad,y_val_pad[:,:-1]], y_val_pad.reshape(y_val_pad.shape[0],y_val_pad.shape[1], 1)[:,1:])\n",
    "    #validation_split=0.1,\n",
    "    )\n",
    " \n",
    "#Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvZgxhtYp5AY"
   },
   "source": [
    "Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoFU_xuzp7nj"
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPBvSyGNqDlq"
   },
   "source": [
    "Implement Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPe0pDCJqGIF"
   },
   "outputs": [],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08_8l-KVqL0F"
   },
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=model.fit([X_train_pad,y_train_pad[:,:-1]], \n",
    "                #  y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)[:,1:],\n",
    "                 # epochs=50,callbacks=[es],batch_size=512, \n",
    "                  #validation_data=([X_val_pad,y_val_pad[:,:-1]], \n",
    "                   #                y_val_pad.reshape(y_val_pad.shape[0], y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuCD-guTqOdU"
   },
   "outputs": [],
   "source": [
    "#history=model.fit([X_train_pad,y_train_pad[:,:-1]], \n",
    " #                 y_train_pad.reshape(y_train_pad.shape[0], y_train_pad.shape[1], 1)[:,1:],\n",
    "  #                epochs=50,callbacks=[es],batch_size=512, \n",
    "   #               validation_data=([X_val_pad,y_val_pad[:,:-1]], \n",
    "    #                               y_val_pad.reshape(y_val_pad.shape[0], y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_njDl36rgcV"
   },
   "source": [
    "Diagnostics enabling us to check the course of losses for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx2m8536reeq"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=X_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGZwLW2uuYJk"
   },
   "source": [
    "Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H43DR6GrfBX"
   },
   "outputs": [],
   "source": [
    "# encoder inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attention = model.layers[8]\n",
    "attn_out_inf = attention([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "#decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_state_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6XYmfk_uvh9"
   },
   "source": [
    "Function implementing the Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdAbshrtupvu"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhaan0Flvj3n"
   },
   "source": [
    "Function to transform Integers back to Words for our source sequence of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I69Av6tiu5u_"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6evXjT7hxIkg"
   },
   "source": [
    "Function to transform Integers back to Words for our target sequence of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0F0tGIdu6FO"
   },
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H1YE2BYxZnF"
   },
   "source": [
    "Show the Output of our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7JJHUkhu6aN"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_val_pad)):\n",
    "    print(\"Review:\",seq2text(X_val_pad[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_val_pad[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(X_val_pad[i].reshape(1,max_len_text)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQqNZb9ru6kt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wJnyNWvu6vL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Jul 15 2022, 11:57:58) \n[GCC 9.4.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "45cd5dc6b295e32d11fa8e4905c3b53a5135ef98e8c5efa32d16c94e39fd63c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
