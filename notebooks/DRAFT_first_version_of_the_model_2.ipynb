{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install / Import all Python Modules Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaPSqT6fPMwt",
    "outputId": "ec96648b-6f78-4a86-f1a4-770f657bbab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\r\n",
      "Requirement already satisfied: pyahocorasick in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGSP1XuGewDX",
    "outputId": "af2f8367-257a-48b9-f314-e02e8e01e0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (0.21.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/christianschultes/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages (from nltk) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ba9EcWEhOvBP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kXHZoChaOu2c"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets required (Train Set, Validation Set, Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "UB7-TFkjOurC",
    "outputId": "9012fe2b-54e8-4f3a-830f-db251790634d"
   },
   "outputs": [],
   "source": [
    "#loading the datasets\n",
    "train_set = pd.read_csv('../raw_data/train.csv')\n",
    "validation_set = pd.read_csv('../raw_data/validation.csv')\n",
    "test_set = pd.read_csv('../raw_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "SGYMbKSTOudV",
    "outputId": "66d47752-305c-485d-d258-ccd2339b710b"
   },
   "outputs": [],
   "source": [
    "# droppping useless columns\n",
    "train_data = train_set.drop(['id', 'orig_id'], axis=1)[:800]\n",
    "val_data = validation_set.drop(['id', 'orig_id'], axis=1)[:200]\n",
    "test_data = test_set.drop(['id', 'orig_id'], axis=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5Eo0oP9mOuSM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gZLUT0l6Ot9S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BAGHDAD, Iraq (CNN) -- The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table for their children -- for as little as $8 a day. Suha, 37, is a mother of three. She says her husband thinks she is cleaning houses when she leaves home. \"People shouldn't criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but they never ask why we had to take this path.\" A mother of three, she wears light makeup, a gold pendant of Iraq around her neck, and an unexpected air of elegance about her. \"I don't have money to take my kid to the doctor. I have to do anything that I can to preserve my child, because I am a mother,\" she says, explaining why she prostitutes herself. Anger and frustration rise in her voice as she speaks. \"No matter what else I may be, no matter how off the path I may be, I am a mother!\"  Watch a woman describe turning to prostitution to \"save my child\" Â» . Her clasped hands clench and unclench nervously. Suha's husband thinks that she is cleaning houses when she goes away. So does Karima's family. \"At the start I was cleaning homes, but I wasn't making much. No matter how hard I worked it just wasn't enough,\" she says. Karima, clad in all black, adds, \"My husband died of lung cancer nine months ago and left me with nothing.\" She has five children, ages 8 to 17. Her eldest son could work, but she's too afraid for his life to let him go into the streets, preferring to sacrifice herself than risk her child. She was solicited the first time when she was cleaning an office. \"They took advantage of me,\" she says softly. \"At first I rejected it, but then I realized I have to do it.\" Both Suha and Karima have clients that call them a couple times a week. Other women resort to trips to the market to find potential clients. Or they flag down vehicles. Prostitution is a choice more and more Iraqi women are making just to survive. \"It's increasing,\" Suha says. \"I found this 'thing' through my friend, and I have another friend in the same predicament as mine. Because of the circumstance, she is forced to do such things.\" Violence, increased cost of living, and lack of any sort of government aid leave women like these with few other options, according to humanitarian workers. \"At this point there is a population of women who have to sell their bodies in order to keep their children alive,\" says Yanar Mohammed, head and founder of the Organization for Women's Freedom in Iraq. \"It's a taboo that no one is speaking about.\" She adds, \"There is a huge population of women who were the victims of war who had to sell their bodies, their souls and they lost it all. It crushes us to see them, but we have to work on it and that's why we started our team of women activists.\" Her team pounds the streets of Baghdad looking for these victims often too humiliated to come forward. \"Most of the women that we find at hospitals [who] have tried to commit suicide\" have been involved in prostitution, said Basma Rahim, a member of Mohammed's team. The team's aim is to compile information on specific cases and present it to Iraq's political parties -- to have them, as Mohammed puts it, \"come tell us what [they] are ... going to do about this.\" Rahim tells the heartbreaking story of one woman they found who lives in a room with three of her children: \"She has sex while her three children are in the room, but she makes them stand in separate corners.\" According to Rahim and Mohammed, most of the women they encounter say they are driven to prostitution by a desperate desire for survival in the dangerously violent and unforgiving circumstances in Iraq. \"They took this path but they are not pleased,\" Rahim says. Karima says when she sees her children with food on the table, she is able to convince herself that it's worth it. \"Everything is for the children. They are the beauty in life and, without them, we cannot live.\" But she says, \"I would never allow my daughter to do this. I would rather marry her off at 13 than have her go through this.\" Karima's last happy memory is of her late husband, when they were a family and able to shoulder the hardships of life in today's Iraq together. Suha says as a young girl she dreamed of being a doctor, with her mom boasting about her potential in that career. Life couldn't have taken her further from that dream. \"It's not like we were born into this, nor was it ever in my blood,\" she says. What she does for her family to survive now eats away at her. \"I lay on my pillow and my brain is spinning, and it all comes back to me as if I am watching a movie.\" E-mail to a friend .</td>\n",
       "      <td>Aid workers: Violence, increased cost of living drive women to prostitution .\\nGroup is working to raise awareness of the problem with Iraq's political leaders .\\nTwo Iraqi mothers tell CNN they turned to prostitution to help feed their children .\\n\"Everything is for the children,\" one woman says .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group's extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC's 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group's sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army's Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they've found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of 'El Negro Acacio' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC's 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia's oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State. E-mail to a friend . Journalist Fernando Ramos contributed to this report.</td>\n",
       "      <td>Tomas Medina Caracas was a fugitive from a U.S. drug trafficking indictment .\\n\"El Negro Acacio\" allegedly helped manage extensive cocaine network .\\nU.S. Justice Department indicted him in 2002 .\\nColombian military: He was killed in an attack on a guerrilla encampment .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               article  \\\n",
       "6  BAGHDAD, Iraq (CNN) -- The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table for their children -- for as little as $8 a day. Suha, 37, is a mother of three. She says her husband thinks she is cleaning houses when she leaves home. \"People shouldn't criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but they never ask why we had to take this path.\" A mother of three, she wears light makeup, a gold pendant of Iraq around her neck, and an unexpected air of elegance about her. \"I don't have money to take my kid to the doctor. I have to do anything that I can to preserve my child, because I am a mother,\" she says, explaining why she prostitutes herself. Anger and frustration rise in her voice as she speaks. \"No matter what else I may be, no matter how off the path I may be, I am a mother!\"  Watch a woman describe turning to prostitution to \"save my child\" Â» . Her clasped hands clench and unclench nervously. Suha's husband thinks that she is cleaning houses when she goes away. So does Karima's family. \"At the start I was cleaning homes, but I wasn't making much. No matter how hard I worked it just wasn't enough,\" she says. Karima, clad in all black, adds, \"My husband died of lung cancer nine months ago and left me with nothing.\" She has five children, ages 8 to 17. Her eldest son could work, but she's too afraid for his life to let him go into the streets, preferring to sacrifice herself than risk her child. She was solicited the first time when she was cleaning an office. \"They took advantage of me,\" she says softly. \"At first I rejected it, but then I realized I have to do it.\" Both Suha and Karima have clients that call them a couple times a week. Other women resort to trips to the market to find potential clients. Or they flag down vehicles. Prostitution is a choice more and more Iraqi women are making just to survive. \"It's increasing,\" Suha says. \"I found this 'thing' through my friend, and I have another friend in the same predicament as mine. Because of the circumstance, she is forced to do such things.\" Violence, increased cost of living, and lack of any sort of government aid leave women like these with few other options, according to humanitarian workers. \"At this point there is a population of women who have to sell their bodies in order to keep their children alive,\" says Yanar Mohammed, head and founder of the Organization for Women's Freedom in Iraq. \"It's a taboo that no one is speaking about.\" She adds, \"There is a huge population of women who were the victims of war who had to sell their bodies, their souls and they lost it all. It crushes us to see them, but we have to work on it and that's why we started our team of women activists.\" Her team pounds the streets of Baghdad looking for these victims often too humiliated to come forward. \"Most of the women that we find at hospitals [who] have tried to commit suicide\" have been involved in prostitution, said Basma Rahim, a member of Mohammed's team. The team's aim is to compile information on specific cases and present it to Iraq's political parties -- to have them, as Mohammed puts it, \"come tell us what [they] are ... going to do about this.\" Rahim tells the heartbreaking story of one woman they found who lives in a room with three of her children: \"She has sex while her three children are in the room, but she makes them stand in separate corners.\" According to Rahim and Mohammed, most of the women they encounter say they are driven to prostitution by a desperate desire for survival in the dangerously violent and unforgiving circumstances in Iraq. \"They took this path but they are not pleased,\" Rahim says. Karima says when she sees her children with food on the table, she is able to convince herself that it's worth it. \"Everything is for the children. They are the beauty in life and, without them, we cannot live.\" But she says, \"I would never allow my daughter to do this. I would rather marry her off at 13 than have her go through this.\" Karima's last happy memory is of her late husband, when they were a family and able to shoulder the hardships of life in today's Iraq together. Suha says as a young girl she dreamed of being a doctor, with her mom boasting about her potential in that career. Life couldn't have taken her further from that dream. \"It's not like we were born into this, nor was it ever in my blood,\" she says. What she does for her family to survive now eats away at her. \"I lay on my pillow and my brain is spinning, and it all comes back to me as if I am watching a movie.\" E-mail to a friend .   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group's extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC's 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group's sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army's Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they've found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of 'El Negro Acacio' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC's 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia's oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State. E-mail to a friend . Journalist Fernando Ramos contributed to this report.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    highlights  \n",
       "6  Aid workers: Violence, increased cost of living drive women to prostitution .\\nGroup is working to raise awareness of the problem with Iraq's political leaders .\\nTwo Iraqi mothers tell CNN they turned to prostitution to help feed their children .\\n\"Everything is for the children,\" one woman says .  \n",
       "7                             Tomas Medina Caracas was a fugitive from a U.S. drug trafficking indictment .\\n\"El Negro Acacio\" allegedly helped manage extensive cocaine network .\\nU.S. Justice Department indicted him in 2002 .\\nColombian military: He was killed in an attack on a guerrilla encampment .  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U2_3nE9PPC11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (800, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_data.shape: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are any Empty Cells in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZCCZf26-PDBA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IU72Gu1tPDNK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DovhJuAzP3Up"
   },
   "source": [
    "## Dealing with Duplicates / Drop Duplicates in the Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TLFV9-VNPDiK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QvcTGPr8PDsX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rH72px5sPD2m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.duplicated(subset=['article', 'highlights']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3mRd9-vZPEAI"
   },
   "outputs": [],
   "source": [
    "def del_duplicates(dataset, columns_to_compare):\n",
    "    '''\n",
    "    Function that deletes duplicated lines comapres according to indicated columns\n",
    "    '''\n",
    "    return dataset.drop_duplicates(subset=columns_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VvulFo2SQARj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['article', 'highlights']\n",
    "\n",
    "train = del_duplicates(train_data, columns_to_compare=cols)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ifg14tFdQAbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = del_duplicates(val_data, columns_to_compare=cols)\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K0I2DFGnQAlV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = del_duplicates(test_data, columns_to_compare=cols)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-obtjbnQMJn"
   },
   "source": [
    "## Potential threshhold for news extraction / Check the Statistical Distribution of Word Counts for Articles and Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Wk1yYboZQA4N"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# word count\n",
    "article_wc = []\n",
    "summary_wc = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in train.article:\n",
    "      article_wc.append(len(i.split()))\n",
    "\n",
    "for i in train.highlights:\n",
    "      summary_wc.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':article_wc, 'summary':summary_wc})\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(length_df.text, bins=40)\n",
    "plt.xlabel(\"Word count\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(length_df.summary, bins=40)\n",
    "plt.xlabel(\"Word count\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OToRGc75QTef"
   },
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Function \"Preprocessing\" to perform the single cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform upper cases to lower cases\n",
    "- Remove return characters, url and html tags\n",
    "- Expand shortened words via Contractions\n",
    "- Remove any parentheses with text inside\n",
    "- Remove special characters, remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DaUHSqf9QBJ6"
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence: string, remove_stopwords=True) -> string: \n",
    "    \n",
    "    '''\n",
    "    Preprocessing text: lower case, \n",
    "                        deleting punctuation, \n",
    "                        replacing contructions with equivalent,\n",
    "                        deleting stop words,\n",
    "                        removing special characters\n",
    "    '''\n",
    "\n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "   \n",
    "    # Remove return characters, url and html tags\n",
    "    code_list = ['\\n', '\\S*(http|https)\\S*', '\\<a href', '&amp;', '<br />']\n",
    "    for code in code_list:\n",
    "        sentence = re.sub(code, ' ',sentence, flags=re.MULTILINE)\n",
    "    \n",
    "    # expand the shortened words (can't => can not)\n",
    "    # after they will be deleted in stopwords\n",
    "    expanded = []   \n",
    "    for word in sentence.split():\n",
    "        expanded.append(contractions.fix(word, slang=False))\n",
    "        \n",
    "    expanded_sentence = ' '.join(expanded)\n",
    "    \n",
    "    # remove any parenthisis with text inside\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', expanded_sentence)\n",
    "        # Removing punctuation, url and html tags\n",
    "    for punctuation in string.punctuation + '[\\'\\\"]':\n",
    "        sentence = sentence.replace(punctuation, ' ')\n",
    "        \n",
    "    # remove special characters \n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "\n",
    "    # Removing whitespaces\n",
    "    sentence = sentence.strip()\n",
    "                \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english')) ## defining stopwords    \n",
    "        sentence_list = [w for w in sentence.split() if not w in stop_words]\n",
    "        sentence = (' '.join(sentence_list)).strip()\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGDtjF4bQhjX"
   },
   "source": [
    "### Test the function \"Preprocessing\" prepared to clean the data on a subset to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gO80ix1OQWFi"
   },
   "outputs": [],
   "source": [
    "def cleaning(dataset: pd.Series, remove_stopwords=True) -> list:\n",
    "    '''\n",
    "    This function creates a cleaned version of each dataset.\n",
    "    Calls the preprocessing function.\n",
    "    '''\n",
    "    \n",
    "    clean = []\n",
    "    for text in dataset:\n",
    "        clean.append(preprocessing(text, remove_stopwords=True))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BKamJPHWQWPE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 s, sys: 115 ms, total: 2.96 s\n",
      "Wall time: 3.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train = cleaning(train.article)\n",
    "y_train = cleaning(train.highlights, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RmrmvZSZQWXn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean article : london england harry potter star daniel radcliffe gains access reported million fortune turns monday insists money cast spell daniel radcliffe harry potter harry potter order phoenix disappointment gossip columnists around world young actor says plans fritter cash away fast cars drink celebrity parties plan one people soon turn suddenly buy massive sports car collection something similar told australian interviewer earlier month think particularly extravagant things like buying things cost pounds books cds dvds radcliffe able gamble casino buy drink pub see horror film hostel part ii currently six places number one movie uk box office chart details mark landmark birthday wraps agent publicist comment plans definitely sort party said interview hopefully none reading radcliffe earnings first five potter films held trust fund able touch despite growing fame riches actor says keeping feet firmly ground people always looking say kid star goes rails told reporters last month try hard go way would easy latest outing boy wizard harry potter order phoenix breaking records sides atlantic reprise role last two films watch reporter give review potter latest life beyond potter however londoner filmed tv movie called boy jack author rudyard kipling son due release later year also appear december boys australian film four boys escape orphanage earlier year made stage debut playing tortured teenager peter shaffer equus meanwhile braced even closer media scrutiny legally adult think going sort fair game told reuters e mail friend copyright reuters rights reserved material may published broadcast rewritten redistributed\n",
      "\n",
      "\n",
      "Clean summary : harry potter star daniel radcliffe gets fortune turns monday young actor says plans fritter cash away radcliffe earnings first five potter films held trust fund\n"
     ]
    }
   ],
   "source": [
    "print(f'Clean article : {X_train[0]}')\n",
    "print('\\n')\n",
    "print(f'Clean summary : {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BFEXlGzDQWg7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 661 ms, sys: 27.4 ms, total: 688 ms\n",
      "Wall time: 734 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_val = cleaning(val.article)\n",
    "y_val = cleaning(val.highlights, remove_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ps5m8v4OQpdY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 383 ms, sys: 19.1 ms, total: 402 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_test = cleaning(test.highlights, remove_stopwords=False)\n",
    "y_test = cleaning(test.article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85bGfFjNQxhV"
   },
   "source": [
    "### !!! Only for target data => adding \"start\" and \"stop\" to the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "s5W6W_maQpvz"
   },
   "outputs": [],
   "source": [
    "def adding_decoder_tokens(data: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Adding special tokens for the decoder only to target string\n",
    "    '''\n",
    "    \n",
    "    return pd.Series(data).apply(lambda x : '_START_ '+ x + ' _END_')\n",
    "\n",
    "y_train = adding_decoder_tokens(y_train)\n",
    "y_val = adding_decoder_tokens(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "H7Z0Y7kQQp4G"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_START_ harry potter star daniel radcliffe gets fortune turns monday young actor says plans fritter cash away radcliffe earnings first five potter films held trust fund _END_'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYq42bubQ63S"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the max len for Article and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yl7bnjFEQ4BO"
   },
   "outputs": [],
   "source": [
    "# initialize the max len for article and summary\n",
    "max_len_text = 80\n",
    "max_len_summary= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1YDlcmnRAjB"
   },
   "source": [
    "### Transform each Article in articles to a sequence of Integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this step are the lists X_train_tok and X_val_tok that basically are lists of tokenized articles in turn being lists of words transformed to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sGkx77RsQ4WP"
   },
   "outputs": [],
   "source": [
    "# learning the dictionnary from train articles\n",
    "X_tokenizer = Tokenizer()\n",
    "X_tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# Transforms each article in articles to a sequence of integers.\n",
    "X_train_tok = X_tokenizer.texts_to_sequences(X_train) \n",
    "X_val_tok = X_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P8GHEt9MQ4f-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized article looks like this : \n",
      "[226, 334, 1145, 4571, 608, 1733, 4572, 3940, 1175, 103, 88, 4573, 2458, 73, 5429, 257, 1927, 4976, 1733, 4572, 1145, 4571, 1145, 4571, 538, 3294, 4224, 11147, 14881, 115, 33, 282, 2159, 25, 490, 14882, 1680, 160, 1211, 888, 3475, 2359, 1611, 409, 2, 3, 589, 644, 4225, 982, 1212, 1612, 157, 2459, 171, 1039, 11, 1176, 11148, 162, 117, 38, 1007, 14883, 203, 23, 3476, 203, 799, 1520, 2073, 11149, 9049, 4572, 255, 11150, 4574, 982, 3475, 11151, 53, 3941, 1213, 11152, 101, 1803, 1476, 124, 1040, 185, 2, 950, 843, 1316, 152, 6720, 1248, 861, 5976, 1997, 7752, 1389, 9050, 914, 490, 2568, 1568, 111, 1, 583, 3694, 1430, 2460, 4572, 7753, 18, 121, 4571, 2960, 228, 951, 1431, 255, 2360, 596, 830, 4977, 7754, 2159, 25, 1928, 706, 7755, 417, 3, 283, 500, 46, 2461, 608, 844, 14884, 11, 501, 19, 117, 491, 340, 89, 65, 4, 952, 889, 6721, 590, 11153, 1145, 4571, 538, 3294, 1613, 1317, 1477, 2569, 14885, 479, 19, 6, 2960, 26, 1064, 284, 1734, 4571, 889, 76, 1249, 4571, 247, 14886, 3942, 766, 950, 80, 590, 1041, 2252, 14887, 14888, 223, 928, 341, 134, 9, 7, 983, 609, 1998, 1176, 1213, 70, 1998, 1614, 6722, 162, 9, 45, 1288, 4978, 813, 4979, 2074, 2253, 14889, 14890, 645, 14891, 84, 1999, 280, 3477, 3113, 1390, 38, 68, 1568, 1115, 233, 11, 1478, 12, 16, 13, 781, 1478, 172, 800, 876, 48, 716, 984, 1352, 1353]\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized article looks like this : ')\n",
    "print(X_train_tok[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the Tokenized Articles to max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The padding step shall ensure that all tokenized articles are split in a way that the results are lists of integers all having the same length max_len_text\n",
    "- If a tokenized article is longer than max_len_text it is truncated\n",
    "- If a tokenized article is shorter than max_len_text it is filled with 0's to reach a length of max_len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XaRpUF6UQWpO"
   },
   "outputs": [],
   "source": [
    "# post-padding with zeros up to maximum length\n",
    "X_train_pad = pad_sequences(X_train_tok, dtype='float32', maxlen=max_len_text, padding='post') \n",
    "X_val_pad = pad_sequences(X_val_tok, dtype='float32', maxlen=max_len_text, padding='post')\n",
    "\n",
    "X_vocab = len(X_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0uj0QmuwRHCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train articles vocabulary is 24267\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of train articles vocabulary is {X_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbWq5nCdRbhq"
   },
   "source": [
    "## Summary Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Summaries to lists of Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "d8L4ZtTpRHSm"
   },
   "outputs": [],
   "source": [
    "# learning the dictionnary from train summaries\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "# Transforms each summary in summaries to a sequence of integers.\n",
    "y_train_tok = y_tokenizer.texts_to_sequences(y_train) \n",
    "y_val_tok = y_tokenizer.texts_to_sequences(y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the tokenized Summaries to a Length of max_len_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "60Evu6O4RHeV"
   },
   "outputs": [],
   "source": [
    "# post-padding with zeros up to maximum length\n",
    "y_train_pad = pad_sequences(y_train_tok, dtype='float32', maxlen=max_len_summary, padding='post')\n",
    "y_val_pad = pad_sequences(y_val_tok,  dtype='float32', maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_vocab = len(y_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "KLKOzFNKRH_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train summary vocabulary is 6821\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of train summary vocabulary is {y_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model that will be Trained and will Predict Summaries of Articles based on the Training Performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules Potentially Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5BHCPHk3Zfq_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K \n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define latent_dim & embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- latent_dim is the dimension intended for the output of the Encoder, i.e. the first step of the Model\n",
    "- embedding_dim is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "k8-MdhYBo9oC"
   },
   "outputs": [],
   "source": [
    "latent_dim = 500\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmpIHCQqRxdk"
   },
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jzYbuaHh0XH"
   },
   "source": [
    "## Step 1 of the Model: Embed the Inputs to the Encoder into the Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Encoder allocates the integers (being included in the tokenized and padded lists of integers) inserted into the Model into Vectors in the \"Latent Space\"\n",
    "- Input shape is max_len_text as via the step before we have padded the lists of integers to a length of max_len_text\n",
    "- X_Vocab is the length of the X_Tokenizer Word Index + 1, i.e. the number of integers + 1. Why \"+1\"? xxx\n",
    "- latent_dim is the dimensionality of the output space\n",
    "- The result of the Encoder, \"enc_emb\", is a Tensor having the shape (None, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "t7Zf8IJdhlpc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:03:10.122641: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Add documentation for encoder\n",
    "# shape: max_len_text, i.e. the maximum length of words of the input text we will insert into the model, correct?\n",
    "# Embedding: X_vocab is the length of the X_Tokenizer Word Index + 1\n",
    "# Embedding: latent_dim is the dimensionality of the output space\n",
    "\n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(X_vocab, latent_dim,trainable=True)(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'embedding')>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJ9TKv9zib6y"
   },
   "source": [
    "## Step 2 of the Model: Three Stages of Stacked Long Short Term Memory (LSTM) Acting as the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three LSTM layers\n",
    "\n",
    "The outputs of this step are\n",
    "\n",
    "- encoder_outputs being a tensor with shape (None, max_len_text, max_len_text)\n",
    "- state_h being a tensor with shape (none, max_len_text)\n",
    "- state_c being a tensor with shape (none, max_len_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "KRFKSPDjiaEI"
   },
   "outputs": [],
   "source": [
    "# LSTM 1 \n",
    "# first integer shown in the brackets is the \"dimensionality of the output space\". so, that would be the length of the output summary, right?\n",
    "# return_sequences = Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "# return_state = Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'lstm')>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'lstm_1')>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'lstm_2')>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 500) dtype=float32 (created by layer 'lstm_2')>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 500) dtype=float32 (created by layer 'lstm_2')>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'lstm_2')>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDWYA9T-is-v"
   },
   "source": [
    "## Step 3 of the Model: Setup the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "VoYbT4rAia4x"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_vocab, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 500) dtype=float32 (created by layer 'embedding_1')>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsTzUy0cjgAN"
   },
   "source": [
    "Decoder based on encoder_states as initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MBx3LRh5jkeV"
   },
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNipeqwZj5lB"
   },
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "OUd41C8_jlMc"
   },
   "outputs": [],
   "source": [
    "#attn_layer = Attention(name='attention_layer') \n",
    "#attn_out = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "attention = Attention(name='attention_layer')\n",
    "attn_out = attention([decoder_outputs, encoder_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 500) dtype=float32 (created by layer 'lstm_3')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'lstm_2')>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQuvnGixkDfM"
   },
   "source": [
    "Concatenate attention output and decoder LSTM output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "dCjlmO-VkGuW"
   },
   "outputs": [],
   "source": [
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 1000) dtype=float32 (created by layer 'concat_layer')>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_concat_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M93_0V5TkbkJ"
   },
   "source": [
    "Add a Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Xu59RA5pkHDh"
   },
   "outputs": [],
   "source": [
    "#decoder_dense = Dense(y_vocab, activation='softmax')\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab, activation='softmax')) \n",
    "#decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 6821) dtype=float32 (created by layer 'time_distributed')>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srb_Kzsikj1S"
   },
   "source": [
    "Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "hXFVF-5xkHX7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 80, 500)      12133500    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 80, 500),    2002000     ['embedding[0][0]']              \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 80, 500),    2002000     ['lstm[0][0]']                   \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 500)    3410500     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 80, 500),    2002000     ['lstm_1[0][0]']                 \n",
      "                                 (None, 500),                                                     \n",
      "                                 (None, 500)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 500),  2002000     ['embedding_1[0][0]',            \n",
      "                                 (None, 500),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 500)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_layer (Attention)    (None, None, 500)    0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 1000)   0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 6821)  6827821     ['concat_layer[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 30,379,821\n",
      "Trainable params: 30,379,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:03:20.903252: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 4194304 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 8.8247 - accuracy: 0.0289      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:05:20.763165: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 4194304 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 136s 61s/step - loss: 8.8247 - accuracy: 0.0289 - val_loss: 8.1368 - val_accuracy: 0.1111\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 190s 48s/step - loss: 8.1139 - accuracy: 0.1114 - val_loss: 8.2866 - val_accuracy: 0.0130\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 169s 43s/step - loss: 7.6987 - accuracy: 0.0411 - val_loss: 8.1580 - val_accuracy: 0.1111\n",
      "Epoch 3: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:12:03.840741: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x12dfc5f70> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x12e04cd30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x12df93dc0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x12dfc55b0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.compile( \n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
    "history = model.fit( \n",
    "    [X_train_pad,y_train_pad[:,:-1]],\n",
    "    y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)[:,1:], \n",
    "    batch_size=512, \n",
    "    epochs=10,\n",
    "    callbacks=[es],\n",
    "    validation_split=0.1,\n",
    "    )\n",
    " \n",
    "#Save model\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvZgxhtYp5AY"
   },
   "source": [
    "Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "eoFU_xuzp7nj"
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPBvSyGNqDlq"
   },
   "source": [
    "Implement Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "yPe0pDCJqGIF"
   },
   "outputs": [],
   "source": [
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08_8l-KVqL0F"
   },
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=model.fit([X_train_pad,y_train_pad[:,:-1]], \n",
    "                #  y_train_pad.reshape(len(y_train_pad), max_len_summary, 1)[:,1:],\n",
    "                 # epochs=50,callbacks=[es],batch_size=512, \n",
    "                  #validation_data=([X_val_pad,y_val_pad[:,:-1]], \n",
    "                   #                y_val_pad.reshape(y_val_pad.shape[0], y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "VuCD-guTqOdU"
   },
   "outputs": [],
   "source": [
    "#history=model.fit([X_train_pad,y_train_pad[:,:-1]], \n",
    " #                 y_train_pad.reshape(y_train_pad.shape[0], y_train_pad.shape[1], 1)[:,1:],\n",
    "  #                epochs=50,callbacks=[es],batch_size=512, \n",
    "   #               validation_data=([X_val_pad,y_val_pad[:,:-1]], \n",
    "    #                               y_val_pad.reshape(y_val_pad.shape[0], y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_njDl36rgcV"
   },
   "source": [
    "Diagnostics enabling us to check the course of losses for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "jx2m8536reeq"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=X_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGZwLW2uuYJk"
   },
   "source": [
    "Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "7H43DR6GrfBX"
   },
   "outputs": [],
   "source": [
    "# encoder inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attention = model.layers[8]\n",
    "attn_out_inf = attention([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "#decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 500) dtype=float32 (created by layer 'lstm_3')>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 500)    3410500     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 500),  2002000     ['embedding_1[1][0]',            \n",
      "                                 (None, 500),                     'input_3[0][0]',                \n",
      "                                 (None, 500)]                     'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 80, 500)]    0           []                               \n",
      "                                                                                                  \n",
      " attention_layer (Attention)    (None, None, 500)    0           ['lstm_3[1][0]',                 \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " concat (Concatenate)           (None, None, 1000)   0           ['lstm_3[1][0]',                 \n",
      "                                                                  'attention_layer[1][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 6821)  6827821     ['concat[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,240,321\n",
      "Trainable params: 12,240,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None, 500) dtype=float32 (created by layer 'lstm_3')>,\n",
       " <KerasTensor: shape=(None, 500) dtype=float32 (created by layer 'lstm_3')>,\n",
       " <KerasTensor: shape=(None, 500) dtype=float32 (created by layer 'lstm_3')>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 80, 500) dtype=float32 (created by layer 'input_5')>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden_state_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6XYmfk_uvh9"
   },
   "source": [
    "Function implementing the Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "IdAbshrtupvu"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhaan0Flvj3n"
   },
   "source": [
    "Function to transform Integers back to Words for our source sequence of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "I69Av6tiu5u_"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6evXjT7hxIkg"
   },
   "source": [
    "Function to transform Integers back to Words for our target sequence of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "M0F0tGIdu6FO"
   },
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H1YE2BYxZnF"
   },
   "source": [
    "Show the Output of our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "-7JJHUkhu6aN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: list live donor came along someone nice enough give away kidney stranger jacobs paid forward programming skills creating program genetically matches donor pairs chains quickly five way swap years ago one largest took three four months three weeks jacobs said chain would worked quickly without generosity may worked significance altruistic donor opens possibilities donors recipients said dr steven three four options inclusion altruistic donor options consider matching donors recipients divine friend shirley williams wrote comment facebook page true angel friend \n",
      "Original summary: give kidney new computer program helped six kidney patients \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 21:12:31.333459: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 1200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 4194304 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview:\u001b[39m\u001b[38;5;124m\"\u001b[39m,seq2text(X_val_pad[i]))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,seq2summary(y_val_pad[i]))\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_pad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_len_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [73], line 14\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     12\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m---> 14\u001b[0m     output_tokens, h, c \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43me_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_c\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Sample a token\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     sampled_token_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output_tokens[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/keras/engine/training.py:1951\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1944\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1949\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 1951\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/keras/engine/data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/keras/engine/data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/keras/engine/data_adapter.py:326\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[1;32m    324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m--> 326\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2060\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2027\u001b[0m   \u001b[38;5;124;03m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m \n\u001b[1;32m   2029\u001b[0m \u001b[38;5;124;03m  The type signature is:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2060\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:5279\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m   5277\u001b[0m \u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m   5278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m-> 5279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[1;32m   5282\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5283\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5284\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_get_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3070\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3062\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   3063\u001b[0m \n\u001b[1;32m   3064\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3070\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3072\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3036\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 3036\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   3038\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   3039\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3292\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3289\u001b[0m       args, kwargs, flat_args, filtered_flat_args)\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd_call_context(cache_key\u001b[38;5;241m.\u001b[39mcall_context)\n\u001b[0;32m-> 3292\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                          graph_function)\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3125\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3127\u001b[0m ]\n\u001b[1;32m   3128\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3129\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3142\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3143\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1165\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1161\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m-> 1165\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m check_mutation(func_args_before, func_args, original_func)\n\u001b[1;32m   1169\u001b[0m check_mutation(func_kwargs_before, func_kwargs, original_func)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1124\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.convert\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be compatible with tf.function, Python functions \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return zero or more Tensors or ExtensionTypes or None \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues; in compilation of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(python_func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which is not a Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_control_dependencies:\n\u001b[0;32m-> 1124\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mdeps_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmark_as_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py:250\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.mark_as_return\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    245\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_array_ops\u001b[38;5;241m.\u001b[39mbuild_ta_with_new_flow(tensor, flow)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# We want to make the return values depend on the stateful operations, but\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# we don't want to introduce a cycle, so we make the return value the result\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# of a new identity operation that the stateful operations definitely don't\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# depend on.\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_returned_tensors\u001b[38;5;241m.\u001b[39madd(tensor)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:287\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    284\u001b[0m   \u001b[38;5;66;03m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[1;32m    285\u001b[0m   \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_handle_data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:4077\u001b[0m, in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4075\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   4076\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m-> 4077\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4078\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIdentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4079\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:757\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m callback_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m callback_outputs\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_structure, op_def\u001b[38;5;241m.\u001b[39mis_stateful, op, outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/contextlib.py:120\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/brief-news/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:519\u001b[0m, in \u001b[0;36mFuncGraph.as_default.<locals>.inner_cm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device_function_stack \u001b[38;5;241m=\u001b[39m old_device_stack\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creator_stack \u001b[38;5;241m=\u001b[39m old_creator_stack\n\u001b[0;32m--> 519\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39m_graph_key \u001b[38;5;241m=\u001b[39m old_graph_key\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(len(X_val_pad)):\n",
    "    print(\"Review:\",seq2text(X_val_pad[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_val_pad[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(X_val_pad[i].reshape(1,max_len_text)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQqNZb9ru6kt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wJnyNWvu6vL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
